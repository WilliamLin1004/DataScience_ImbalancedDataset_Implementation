# -*- coding: utf-8 -*-
"""DS_HW2_104403553_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yR6PwLf6sr3cdXtw01wec2wJLRaETRDN

# Introduction
    

> IIn this tutorial, we'll work on two datasets, the creditcard fraud dataset and the boys an girls dataset from class. We'll be focusing on the methods of sampling and different algorithms for classfication. 


---

> A couple data preprocessing methods have been used to enhance performance, including SMOTE from oversampling and random undersampling.Numerous classification methods have also been used.



---


    
> A discussion on the effects of these classifiers and sampling methods is included at the end of this tutorial.



---

> The measurement used for measuring a models perfomance will be f-score, recall and precision.

---

# Research Questions


*   What sampling method get a better performance?
*   Which classifer is better for dealing with inbalanced dataset?
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import pandas as pd
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""# Credit Card Fraud

## Preprocessing

### Read creditcard fraud
"""

link = 'https://drive.google.com/open?id=15Wwy8SP1OYVhz9X8HjoaPOcwm7nUsNUy'
fluff, id = link.split('=')
print (id) # Verify that you have everything after '='
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('creditcard.csv')  
df = pd.read_csv('creditcard.csv')
df = df.dropna()
df.info()
df.head()

"""### Encode Class column"""

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
encoded_Label = label_encoder.fit_transform(df['Class'])
df['Class'] = encoded_Label
df.head()
#df.groupby('class').count()

"""### Divide CreditCardFraud into Features and Label"""

x=df.loc[:,["Time","V1","V2","V3","V4","V5","V6","V7","V8","V9","V10","V11","V12","V13","V14","V15","V16","V17","V18","V19","V20","V21","V22","V23","V24","V25","V26","V27","V28","Amount"]]
y=df.loc[:,['Class']]
#print(y)
#y.count()
#pd.value_counts(y.values.flatten())

"""### Split train and test sets"""

from sklearn.model_selection import train_test_split
from collections import Counter

train_X, test_X, train_y, test_y = train_test_split(x, y, test_size = 0.33, random_state = 1)
print("before smote")
print(str(pd.value_counts(train_y.values.flatten())))

"""### Oversampling 
SMOTE is used
"""

from imblearn.over_sampling import SMOTE
from imblearn.ensemble import EasyEnsemble 
from imblearn.over_sampling import RandomOverSampler

sm = SMOTE(random_state=10000)
#sm1 = SMOTE(random_state=100)
train_X_res, train_y_res = sm.fit_sample(train_X, train_y)
#test_X_res, test_y_res = sm1.fit_sample(test_X, test_y)
#ee = EasyEnsemble(random_state=42)
#X_train_res, y_train_res = ee.fit_sample(train_X, train_y)


train_y_res = pd.DataFrame(train_y_res, columns = ["Class"])
train_X_res = pd.DataFrame(train_X_res, columns = ["Time","V1","V2","V3","V4","V5","V6","V7","V8","V9","V10","V11","V12","V13","V14","V15","V16","V17","V18","V19","V20","V21","V22","V23","V24","V25","V26","V27","V28","Amount"])
#test_y_res = pd.DataFrame(test_y_res, columns = ["Class"])
#test_X_res = pd.DataFrame(test_X_res, columns = ["Time","V1","V2","V3","V4","V5","V6","V7","V8","V9","V10","V11","V12","V13","V14","V15","V16","V17","V18","V19","V20","V21","V22","V23","V24","V25","V26","V27","V28","Amount"])
print(str(pd.value_counts(train_y_res.values.flatten())))
#X_train_res.head()
#print(y_train_res)

"""## Classification

### Decision  Tree
"""

from sklearn import tree
from sklearn import metrics
import graphviz
print('decision tree')
tree_clf_no = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8)
tree_clf_with = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8)
tree_clf_no_smote = tree_clf_no.fit(train_X,train_y)
tree_clf_with_smote = tree_clf_with.fit(train_X_res,train_y_res)
tree_predicted_no_smote = tree_clf_no_smote.predict(test_X)
tree_predicted_with_smote = tree_clf_with_smote.predict(test_X)

tree_accuracy_no_smote = metrics.accuracy_score(tree_predicted_no_smote, test_y)
tree_accuracy_with_smote = metrics.accuracy_score(tree_predicted_with_smote, test_y)
print('Accuracy no smote  ',tree_accuracy_no_smote)
print("Accuracy with smote",tree_accuracy_with_smote)
print("Classification no smote")
print(metrics.classification_report(test_y, tree_predicted_no_smote))
print("Classification with smote")
print(metrics.classification_report(test_y, tree_predicted_with_smote))

"""### Logistic Regression"""

#print("Logistic Regression")
from sklearn.linear_model import LogisticRegression as LR
print("Logistic Regression")
LR_clf_no_smote = LR(solver = 'liblinear').fit(train_X , train_y)
LR_clf_with_smote = LR(solver = 'liblinear').fit(train_X_res , train_y_res)

LR_predicted_no_smote = LR_clf_no_smote.predict(test_X)
LR_predicted_with_smote = LR_clf_with_smote.predict(test_X)

LR_accuracy_no_smote = metrics.accuracy_score(LR_predicted_no_smote, test_y)
LR_accuracy_with_smote = metrics.accuracy_score(LR_predicted_with_smote, test_y)
print('Accuracy no smote',LR_accuracy_no_smote)
print("Accuracy with smote",LR_accuracy_with_smote)
print("Classification no smote  ")
print(metrics.classification_report(test_y, LR_predicted_no_smote))
print("Classification with smote")
print(metrics.classification_report(test_y, LR_predicted_with_smote))

"""### Naive Bayes
Naive Bayes was said to get awful results in our research project<br>
Let's just give it a chance
"""

from sklearn.naive_bayes import GaussianNB
print("Shitty Naive Bayes")
nb_model_no = GaussianNB()
nb_model_with = GaussianNB()
nb_clf_no_smote = nb_model_no.fit(train_X,train_y)
nb_clf_with_smote = nb_model_with.fit(train_X_res, train_y_res)

nb_predicted_no_smote = nb_clf_no_smote.predict(test_X)
nb_predicted_with_smote = nb_clf_with_smote.predict(test_X)

nb_accuracy_no_smote = metrics.accuracy_score(nb_predicted_no_smote, test_y)
nb_accuracy_with_smote = metrics.accuracy_score(nb_predicted_with_smote, test_y)
print('Accuracy no smote',nb_accuracy_no_smote)
print("Accuracy with smote",nb_accuracy_with_smote)
print("Classification no smote  ")
print(metrics.classification_report(test_y, nb_predicted_no_smote))
print("Classification with smote")
print(metrics.classification_report(test_y, nb_predicted_with_smote))

"""### Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

print('Adaboost')
bdt_model_no = AdaBoostClassifier(random_state = 1)
bdt_model_with = AdaBoostClassifier(random_state = 1)
bdt_clf_no_smote = bdt_model_no.fit(train_X,train_y)
bdt_clf_with_smote = bdt_model_with.fit(train_X_res, train_y_res)

bdt_predicted_no_smote = bdt_clf_no_smote.predict(test_X)
bdt_predicted_with_smote = bdt_clf_with_smote.predict(test_X)

bdt_accuracy_no_smote = metrics.accuracy_score(bdt_predicted_no_smote, test_y)
bdt_accuracy_with_smote = metrics.accuracy_score(bdt_predicted_with_smote, test_y)
print('Accuracy no smote',bdt_accuracy_no_smote)
print("Accuracy with smote",bdt_accuracy_with_smote)
print("Classification no smote  ")
print(metrics.classification_report(test_y, bdt_predicted_no_smote))
print("Classification with smote")
print(metrics.classification_report(test_y, bdt_predicted_with_smote))

"""# Boys and Girls

## Input Data
"""

bg_train_link = 'https://drive.google.com/open?id=1zcHuVtGoRe7cGW1wucG3eTf2fxVSIQC1'
bg_test_link = 'https://drive.google.com/open?id=1oafF_fQghX9-kGcfX8zuNuVJnAYtVDej'
fluff1, bg_train_id = bg_train_link.split('=')
fluff1, bg_test_id = bg_test_link.split('=')
#print (id) # Verify that you have everything after '='
bg_train_downloaded = drive.CreateFile({'id':bg_train_id}) 
bg_test_downloaded = drive.CreateFile({'id':bg_test_id})
bg_train_downloaded.GetContentFile('bg_train.csv')  
bg_test_downloaded.GetContentFile('bg_test.csv')  
bg_train_df = pd.read_csv('bg_train.csv')
bg_test_df = pd.read_csv('bg_test.csv')

bg_train_df = bg_train_df.dropna()
bg_test_df = bg_test_df.dropna()



#bg_train_df.info()
#bg_test_df.info()
#bg_train_df.tail()

"""## Preprocessing

### drop columns
drop id, timestamp,sel_intro,yt
"""

bg_train_df = bg_train_df.drop(['id','timestamp','self_intro','yt'],axis = 1)
bg_test_df = bg_test_df.drop(['id','timestamp','self_intro','yt'],axis = 1)

#bg_train_df.head()

"""### Encode
transform nominal data
"""

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
#print(bg_train_df.head())

#bg_train_df['yt'] = bg_train_df.astype('float64')
bg_train_df['star_sign'] =le.fit_transform(bg_train_df['star_sign'])
bg_test_df['star_sign'] = le.fit_transform(bg_test_df['star_sign'])


bg_train_df['phone_os'] =le.fit_transform(bg_train_df['phone_os'])
bg_test_df['phone_os'] = le.fit_transform(bg_test_df['phone_os'])





#bg_train_df.head()
#bg_train_df.info()
#bg_test_df.info()

"""### Outlier Removal"""

bg_train_df =  bg_train_df[(bg_train_df['height']<200) & (bg_train_df['height']>120) & (bg_train_df['weight']<150)&(bg_train_df['weight'])>30 & (bg_train_df['fb_friends']<=5000)]

"""### Feature Label Split"""

bg_train_X = bg_train_df.loc[:,['star_sign','phone_os','height','weight','sleepiness','iq','fb_friends']]
bg_train_y = bg_train_df.loc[:,['gender']]

bg_test_X = bg_test_df.loc[:,['star_sign','phone_os','height','weight','sleepiness','iq','fb_friends']]
bg_test_y = bg_test_df.loc[:,['gender']]


bg_train_X.info()
bg_test_y.info()

print('bg_train before smote:')
print(str(pd.value_counts(bg_train_y.values.flatten())))

"""### Oversampling and Undersampling"""

from imblearn.over_sampling import SMOTE
from imblearn.ensemble import EasyEnsemble 
from imblearn.under_sampling import RandomUnderSampler

bg_sm = SMOTE(random_state=10000)
bg_train_X_res, bg_train_y_res = bg_sm.fit_sample(bg_train_X, bg_train_y)



bg_train_y_res = pd.DataFrame(bg_train_y_res, columns = ["gender"])
bg_train_X_res = pd.DataFrame(bg_train_X_res, columns = ['star_sign','phone_os','height','weight','sleepiness','iq','fb_friends'])

bg_us = RandomUnderSampler(random_state=10000)
bg_train_X_red, bg_train_y_red = bg_us.fit_sample(bg_train_X, bg_train_y)


bg_train_y_red = pd.DataFrame(bg_train_y_red, columns = ["gender"])
bg_train_X_red = pd.DataFrame(bg_train_X_red, columns = ['star_sign','phone_os','height','weight','sleepiness','iq','fb_friends'])



print(str(pd.value_counts(bg_train_y_res.values.flatten())))

"""## Classification

### decision tree
"""

#from sklearn import tree as bg_tree
from sklearn import metrics
import graphviz
print('decision tree')
#tree_clf_no = bg_tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8)
#tree_clf_with = bg_tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8)

bg_tree_clf_no_smote = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8).fit(bg_train_X,bg_train_y)
bg_tree_clf_with_smote = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8).fit(bg_train_X_res,bg_train_y_res)
bg_tree_clf_with_under = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3,max_leaf_nodes = 8).fit(bg_train_X_red,bg_train_y_red)

bg_tree_predicted_no_smote = bg_tree_clf_no_smote.predict(bg_test_X)
bg_tree_predicted_with_smote = bg_tree_clf_with_smote.predict(bg_test_X)
bg_tree_predicted_with_under = bg_tree_clf_with_under.predict(bg_test_X)

#bg_test_y_list = bg_test_y.iloc[:, 0].tolist()
print(bg_test_y.iloc[:, 0].values.flatten())
print(bg_tree_predicted_no_smote)
print(bg_tree_predicted_with_smote)
print(bg_tree_predicted_with_under)


bg_tree_accuracy_no_smote = metrics.accuracy_score(bg_tree_predicted_no_smote, bg_test_y)
bg_tree_accuracy_with_smote = metrics.accuracy_score(bg_tree_predicted_with_smote, bg_test_y)
bg_tree_accuracy_with_under = metrics.accuracy_score(bg_tree_predicted_with_under, bg_test_y)

print('Accuracy no smote  ',bg_tree_accuracy_no_smote)
print("Classification no smote")
print(metrics.classification_report(bg_test_y, bg_tree_predicted_no_smote))


print("Accuracy with smote",bg_tree_accuracy_with_smote)
print("Classification with smote")
print(metrics.classification_report(bg_test_y, bg_tree_predicted_with_smote))

print("Accuracy with under",bg_tree_accuracy_with_under)
print("Classification with under_sampling")
print(metrics.classification_report(bg_test_y, bg_tree_predicted_with_under))

"""### Logistic Regression on Boys and Girls
something went wrong here
"""

#print("Logistic Regression")
from sklearn.linear_model import LogisticRegression as bg_LR

#print(bg_train_y_res.iloc[:, 0].values.flatten())
print("Logistic Regression")

bg_LR_clf_no_smote = bg_LR(solver = 'liblinear').fit(bg_train_X , bg_train_y)
bg_LR_clf_with_smote = bg_LR(solver = 'liblinear').fit(bg_train_X_res , bg_train_y_res)
bg_LR_clf_with_under = bg_LR(solver = 'liblinear').fit(bg_train_X_red , bg_train_y_red)

bg_LR_predicted_no_smote = bg_LR_clf_no_smote.predict(bg_test_X)
bg_LR_predicted_with_smote = bg_LR_clf_with_smote.predict(bg_test_X)
bg_LR_predicted_with_under = bg_LR_clf_with_under.predict(bg_test_X)

print(bg_test_y.iloc[:, 0].values.flatten())
print(bg_LR_predicted_no_smote)
print(bg_LR_predicted_with_smote)
print(bg_LR_predicted_with_under)

bg_LR_accuracy_no_smote = metrics.accuracy_score(bg_LR_predicted_no_smote, bg_test_y)
bg_LR_accuracy_with_smote = metrics.accuracy_score(bg_LR_predicted_with_smote, bg_test_y)
bg_LR_accuracy_with_under = metrics.accuracy_score(bg_LR_predicted_with_under, bg_test_y)



print('Accuracy no smote',bg_LR_accuracy_no_smote)
print("Classification no smote  ")
print(metrics.classification_report(bg_test_y, bg_LR_predicted_no_smote))

print("Accuracy with smote",bg_LR_accuracy_with_smote)
print("Classification with smote")
print(metrics.classification_report(bg_test_y, bg_LR_predicted_with_smote))

print("Accuracy with under",bg_LR_accuracy_with_under)
print("Classification with under")
print(metrics.classification_report(bg_test_y, bg_LR_predicted_with_under))

"""### Naive Bayes 
something went wrong here
"""

#print("Logistic Regression")
from sklearn.naive_bayes import GaussianNB
print("Naive Bayes")
bg_nb_clf_no_smote = GaussianNB().fit(bg_train_X , bg_train_y)
bg_nb_clf_with_smote = GaussianNB().fit(bg_train_X_res , bg_train_y_res)
bg_nb_clf_with_under = GaussianNB().fit(bg_train_X_red , bg_train_y_red)

bg_nb_predicted_no_smote = bg_nb_clf_no_smote.predict(bg_test_X)
bg_nb_predicted_with_smote = bg_nb_clf_with_smote.predict(bg_test_X)
bg_nb_predicted_with_under = bg_nb_clf_with_under.predict(bg_test_X)


print(bg_test_y.iloc[:, 0].values.flatten())
print(bg_nb_predicted_no_smote)
print(bg_nb_predicted_with_smote)
print(bg_nb_predicted_with_under)

bg_nb_accuracy_no_smote = metrics.accuracy_score(bg_nb_predicted_no_smote, bg_test_y)
bg_nb_accuracy_with_smote = metrics.accuracy_score(bg_nb_predicted_with_smote, bg_test_y)
bg_nb_accuracy_with_under = metrics.accuracy_score(bg_nb_predicted_with_under, bg_test_y)

print('Accuracy no smote',bg_nb_accuracy_no_smote)
print("Classification no smote  ")
print(metrics.classification_report(bg_test_y, bg_nb_predicted_no_smote))

print("Accuracy with smote",bg_nb_accuracy_with_smote)
print("Classification with smote")
print(metrics.classification_report(bg_test_y, bg_nb_predicted_with_smote))

print("Accuracy with under",bg_nb_accuracy_with_under)
print("Classification with under")
print(metrics.classification_report(bg_test_y, bg_nb_predicted_with_under))

"""### Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
bg_bdt_no = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),
                         algorithm="SAMME",
                         n_estimators=200)
bg_bdt_with_smote = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),
                         algorithm="SAMME",
                         n_estimators=200)
bg_bdt_with_under = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),
                         algorithm="SAMME",
                         n_estimators=200)

bg_bdt_clf_no_smote = bg_bdt_no.fit(bg_train_X,bg_train_y)
bg_bdt_clf_with_smote = bg_bdt_with_smote.fit(bg_train_X_res,bg_train_y_res)
bg_bdt_clf_with_under = bg_bdt_with_under.fit(bg_train_X_red,bg_train_y_red)

bg_bdt_predicted_no_smote = bg_bdt_clf_no_smote.predict(bg_test_X)
bg_bdt_predicted_with_smote = bg_bdt_clf_with_smote.predict(bg_test_X)
bg_bdt_predicted_with_under = bg_bdt_clf_with_under.predict(bg_test_X)

#bg_test_y_list = bg_test_y.iloc[:, 0].tolist()
print(bg_test_y.iloc[:, 0].values.flatten())
print(bg_bdt_predicted_no_smote)
print(bg_bdt_predicted_with_smote)
print(bg_bdt_predicted_with_under)


bg_bdt_accuracy_no_smote = metrics.accuracy_score(bg_bdt_predicted_no_smote, bg_test_y)
bg_bdt_accuracy_with_smote = metrics.accuracy_score(bg_bdt_predicted_with_smote, bg_test_y)
bg_bdt_accuracy_with_under = metrics.accuracy_score(bg_bdt_predicted_with_under, bg_test_y)

print('Accuracy no smote  ',bg_bdt_accuracy_no_smote)
print("Classification no smote")
print(metrics.classification_report(bg_test_y, bg_bdt_predicted_no_smote))


print("Accuracy with smote",bg_bdt_accuracy_with_smote)
print("Classification with smote")
print(metrics.classification_report(bg_test_y, bg_bdt_predicted_with_smote))

print("Accuracy with under",bg_bdt_accuracy_with_under)
print("Classification with under_sampling")
print(metrics.classification_report(bg_test_y, bg_bdt_predicted_with_under))



"""# Discussion

## Which method of sampling worked better on imbalanced datasets?

### Credit Card Fraud
Only SMOTE is done on this dataset, since the whole dataset is really biased toward the no-fraud section, if we removed too many instances, this dataset might lose some of its properties. <br>
What we can see here is that the performance(f-score, accuracy) of the classifiers all dropped after SMOTE. Recall rate can be seen to have an improvement in all cases, but precision suffer badly as a consequence. This is probably due to the increasing of minority classes, which increased the chance of having the classifier making minority class guesses.

### Boys and girls
Both SMOTE and Undersampling showed a significant improvement on the f-score of the minority class on this dataset using decision tree an logistic regression. However, when using adaboost and naive bayes, the performance is not rewarding at all. In Adaboost, we can see that the performance undersampling is the best. As for naive bayes, something must've went wrong since the results are really weird.

### Conclusion
A proved better sampling method can't be found. It all depends on the property of the dataset.

---

## Which classifier has a better performance?

From the experiment done above, the performance can be seen that Adaboost is a more reliable and consistent method of doing things than all the other algorithms. The decision tree is also one that's is more stable, and have a better performance using sampling methods.As for Naive Bayes and Logistic Regression, the model constructed are either weird or does not prove anything. Therefore, these two methods shouldn't be used when dealing with Inbalanced datasets.

---

## Trade-off
Adaboost takes up a lot of time. If we run Adaboost on the CreditCard fraud, we can go have a cup of tea before a result can be shown. What I mean here is that although Adaboost is consistent, the time for it to get a model is quite long.
"""

